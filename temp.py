# -*- coding: utf-8 -*-
"""temp.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/#fileId=https%3A//storage.googleapis.com/kaggle-colab-exported-notebooks/temp-ipynb-8a01c9dc-e0e9-4b5e-a9ca-9df418a9b9f4.ipynb%3FX-Goog-Algorithm%3DGOOG4-RSA-SHA256%26X-Goog-Credential%3Dgcp-kaggle-com%2540kaggle-161607.iam.gserviceaccount.com/20240920/auto/storage/goog4_request%26X-Goog-Date%3D20240920T100409Z%26X-Goog-Expires%3D259200%26X-Goog-SignedHeaders%3Dhost%26X-Goog-Signature%3D6c49b7890936681d4bf1a15f06cb970261ee28ef6e0f39de5a8a2211d33ab436a54de420382e1a35abd4add53d77230344789979e733dd6cea7f748ed26e5cd054eae562532fc0affbeb6e0836689cf083253a471fa1879b87eb4efdb08d4b36eda155c6a943997d13809970e9b73c1fb1ca637e08f46ce4c975fd69411de54e991fdfbe2b42f2c17b6d021a7edee86856b8f6962bcd3b7a1e1d6f0ee56e1806f11a1cf8f70e899e2261417b26ada041f6505bd56716a610650a3b81b45126315daa85a8f8a3487d176ae1b4ea8ba9e3a09042a2f61c4382e10aaffd7e3d816787575ed707ec6d1c3a860f4623bb2a59479466905434e03b23b4404629fcb2e7
"""

# IMPORTANT: RUN THIS CELL IN ORDER TO IMPORT YOUR KAGGLE DATA SOURCES
# TO THE CORRECT LOCATION (/kaggle/input) IN YOUR NOTEBOOK,
# THEN FEEL FREE TO DELETE THIS CELL.
# NOTE: THIS NOTEBOOK ENVIRONMENT DIFFERS FROM KAGGLE'S PYTHON
# ENVIRONMENT SO THERE MAY BE MISSING LIBRARIES USED BY YOUR
# NOTEBOOK.

# import os
# import sys
# from tempfile import NamedTemporaryFile
# from urllib.request import urlopen
# from urllib.parse import unquote, urlparse
# from urllib.error import HTTPError
# from zipfile import ZipFile
# import tarfile
# import shutil

# CHUNK_SIZE = 40960
# DATA_SOURCE_MAPPING = 'mmsample:https%3A%2F%2Fstorage.googleapis.com%2Fkaggle-data-sets%2F3007402%2F5173923%2Fbundle%2Farchive.zip%3FX-Goog-Algorithm%3DGOOG4-RSA-SHA256%26X-Goog-Credential%3Dgcp-kaggle-com%2540kaggle-161607.iam.gserviceaccount.com%252F20240920%252Fauto%252Fstorage%252Fgoog4_request%26X-Goog-Date%3D20240920T100409Z%26X-Goog-Expires%3D259200%26X-Goog-SignedHeaders%3Dhost%26X-Goog-Signature%3D7c501f70fbed2c94c732483d071a9f5caaaf44096ee76951e6ef7572dc1ebd102e26833a09c6107f7f54aa90aa89b0e91d3a27485e9afad6ee1da51e44367f479f16696a27f41d0b966dcb46b763231fbc65e3b265501f59293194114a4f222dcb9f9fe95c67622f2cb7ddfcd1eef06b4b68ae337f6d4e6da7248b3d15c3723521cfe609ea6d19dac90d8b50eb25bf4ec928fccff86ab42cc3b17e89d8ee3fb37bdfd1d5d6fae954485eea9dc8086a838e217aa2a5603768b9f43eb474f3707cbfe2f4158de4b246dd49ece9fd80bdfa7681308100de41d148d4e08d1a97ba6a9fe5dfeed0651ca3d055745d4da23d3bfffd303df0d7a12736b0feb5561a3ea0,coco-image-caption:https%3A%2F%2Fstorage.googleapis.com%2Fkaggle-data-sets%2F3445072%2F6019472%2Fbundle%2Farchive.zip%3FX-Goog-Algorithm%3DGOOG4-RSA-SHA256%26X-Goog-Credential%3Dgcp-kaggle-com%2540kaggle-161607.iam.gserviceaccount.com%252F20240920%252Fauto%252Fstorage%252Fgoog4_request%26X-Goog-Date%3D20240920T100409Z%26X-Goog-Expires%3D259200%26X-Goog-SignedHeaders%3Dhost%26X-Goog-Signature%3D0861a71167b4df2d3081ce3dcc4fd2534b86f36b82aa344c9bfa1e77571ab59bb780d29676a35581ce81f16654f392c0f418096f4bfc0f064e2396c4029c56ddbccc1531eb6b2ba9b01e206458bf6c2b0ca5413995afdee40b552ceda6dc65172badfd00ceeba89e3749ba7b5fe256b6ee4fe43247ea0ed801ea66cc745f07f1828b103f99db8f7c68ec766c630882d078d09ee8a7be7e3f54044c940f8a5c083dc5265b1896ebcb64efdd8ec26c996890b2824d8c7f45a5615500a55f9c7a833bf5d6bce3e0bef4ee14ca75a30d5907dda706e756a85bf880599a90c4a8203570b345bb23a2d7992686f7568fc8edadea15d1c03d65757ceb0ed7bf1c303b20,clip-val:https%3A%2F%2Fstorage.googleapis.com%2Fkaggle-data-sets%2F5557058%2F9192360%2Fbundle%2Farchive.zip%3FX-Goog-Algorithm%3DGOOG4-RSA-SHA256%26X-Goog-Credential%3Dgcp-kaggle-com%2540kaggle-161607.iam.gserviceaccount.com%252F20240920%252Fauto%252Fstorage%252Fgoog4_request%26X-Goog-Date%3D20240920T100409Z%26X-Goog-Expires%3D259200%26X-Goog-SignedHeaders%3Dhost%26X-Goog-Signature%3D7649e9326978b2e2ce1da5bca2434e23ac0b069c274d19368f4ab579fcc7fcffbf5d2dfe7958c8d8731ab1b2842cfc6840a4c755add513057ffe90bb23b05b02a9dc24e6eed33b342afe4e6a121677ca32486af6437923a80cdd8a9b46d5c46a37f521efc02aa428658b447d61ddd79c7f9a4e783fc0d04497a98c347cbfde32a106c92fda1350567af79cecafdb95532223f5ea44d68a2a3b4ccc9d7eda77136f642acc9af735907de4b17eb153d0f50d04cfc93ba66b15ad2ab729d687a53e61cf683d25641ef6e004ce3432bd6f31dcb310bb9c8173327c58716d3095e6728df84efa6d585a60b602c3e64d5286891810eca17b4e7ce8f4ac017d7f53c9e4,val-dummy:https%3A%2F%2Fstorage.googleapis.com%2Fkaggle-data-sets%2F5565663%2F9205070%2Fbundle%2Farchive.zip%3FX-Goog-Algorithm%3DGOOG4-RSA-SHA256%26X-Goog-Credential%3Dgcp-kaggle-com%2540kaggle-161607.iam.gserviceaccount.com%252F20240920%252Fauto%252Fstorage%252Fgoog4_request%26X-Goog-Date%3D20240920T100409Z%26X-Goog-Expires%3D259200%26X-Goog-SignedHeaders%3Dhost%26X-Goog-Signature%3D5ad9dba8b800340b96b5ffa16c3f1270b44a8632d6f5dc7fe5963bdc698cdf6a38dc135cca5c9667c3b16eb45b40d436221316772857ca409354960b1600403dbe5d2fa10d9e2c25a60504e18410d829307f7decee3fc4255fd40d0a4c213853c18a2b1bbf85ee400c255bcc8cdfcb54bea0a4141bdd54a2e3f37d20df81b3216ba5534d43b48f12c2cc9eceea31f0246de7d0eaf817f9707571eab47084a1ba7d49b94875f7a524baf64278b898d8f9871c9ada55236d90c4ca2e7d9a70e6cbd8813d2754c3fd6fc02556c14e6be58eeeacf51a00a3612585a4e3341a7a2a6ac99698875e464e646b7620f0af6e9faf6eb2c2bfb957870d7809d2b024e9b7c1,whuewi:https%3A%2F%2Fstorage.googleapis.com%2Fkaggle-data-sets%2F5568917%2F9209952%2Fbundle%2Farchive.zip%3FX-Goog-Algorithm%3DGOOG4-RSA-SHA256%26X-Goog-Credential%3Dgcp-kaggle-com%2540kaggle-161607.iam.gserviceaccount.com%252F20240920%252Fauto%252Fstorage%252Fgoog4_request%26X-Goog-Date%3D20240920T100409Z%26X-Goog-Expires%3D259200%26X-Goog-SignedHeaders%3Dhost%26X-Goog-Signature%3D29f0c9484ce1702fa5a69828cb21b98814e6421effd65d36d6fb4e2606b781fe68934126b0fd2cd4b1365c573b0f5e78fe51b6efb6b26cc7ca31498dd9d42a73c22ead58f699e1479246bf642dc89c8051c90474b5592e684b8c4e97f3b4666eeacd54bc1e3349f7de50e487d6f7fb9ae941a7df7ca95d7ba6904587ddb4d6b84553753dab0cf6aa86619e8aca9d3c551f6b5a9cdf0d629e67680d74e6bde5ea7481374a11ae860e6c9998de08ebd6180bb9f71eb3d69982c03a85c6cc02e34e168a2e524377bde667da1cffe91346cda1e8a3bb167452134dada33038c0e6aa1900dc72413933f8a139e141891f28d4aad7fe5de9c1fd9b5244901c25ed3218,bpe-simple-vocab-16e6-txt-zip:https%3A%2F%2Fstorage.googleapis.com%2Fkaggle-data-sets%2F5569939%2F9211475%2Fbundle%2Farchive.zip%3FX-Goog-Algorithm%3DGOOG4-RSA-SHA256%26X-Goog-Credential%3Dgcp-kaggle-com%2540kaggle-161607.iam.gserviceaccount.com%252F20240920%252Fauto%252Fstorage%252Fgoog4_request%26X-Goog-Date%3D20240920T100409Z%26X-Goog-Expires%3D259200%26X-Goog-SignedHeaders%3Dhost%26X-Goog-Signature%3D859867ce40b09ad1e616fc551a1d23ab7b553f8f4867d09e2cd637a10b2e32f4293c71fd0f9f29f9f863abcf6d193527549ff35a663a85a898c525d643701650611c9c74415aa073111a232aca49a413da32b2007b1642cc581b5a4ed30d8946700f546a497836290eb2b87c3fa5e10552c1a417e7006dda88db7de97a65709f29fabf4461f597b727213f0011f81f3d3b1054c52837c935d1f5504a8ccc0739966f51f07fc189363d14756761eac335a6df1de05c0b5f7a6a74b980848b77af651822c1b75ec264edb5d8de4ce4e04db0463c4db6415408500884e2ea946bb9d2a49cd0a1b7258f2f875d0a31be2d31e06019ac62de584ba67680ef36c8c90c,embeddings:https%3A%2F%2Fstorage.googleapis.com%2Fkaggle-data-sets%2F5714444%2F9410570%2Fbundle%2Farchive.zip%3FX-Goog-Algorithm%3DGOOG4-RSA-SHA256%26X-Goog-Credential%3Dgcp-kaggle-com%2540kaggle-161607.iam.gserviceaccount.com%252F20240920%252Fauto%252Fstorage%252Fgoog4_request%26X-Goog-Date%3D20240920T100409Z%26X-Goog-Expires%3D259200%26X-Goog-SignedHeaders%3Dhost%26X-Goog-Signature%3D78e5f228b7c6fa34b6e178c7f95191f5767d5d831bb7f34737f9b40bb04fbf579f1b7a80f5365db5cb48ee8d2c4bb819008ea45064b40db1bd3a3c982e8e13e28b26804659c156e1258d24b5bab82ce930814fb9af2b8b3ba55e5ac41044373554de07b55f945418f92e525b316ccf8a266e2cd35064afe0420daf40e7138c6e50d45220777fdcbc5a0b1bde2ccf6ec5e154815c020ef55a242bf6e0f1ef28aaa383a2ce5e6c83dc26f4e75c9b0d6ace5048f6b5cfe5466a7090c3a58eae9408964c135591f71e64a05377bb15076ab690020412ff6167ff8a4a0c8a09d69b0b00de9aea0d9abc0f92d7b54de87b27851d87a1684cd10082902f355e1f545505'
#
# KAGGLE_INPUT_PATH='/kaggle/input'
# KAGGLE_WORKING_PATH='/kaggle/working'
# KAGGLE_SYMLINK='kaggle'
#
# !umount /kaggle/input/ 2> /dev/null
# shutil.rmtree('/kaggle/input', ignore_errors=True)
# os.makedirs(KAGGLE_INPUT_PATH, 0o777, exist_ok=True)
# os.makedirs(KAGGLE_WORKING_PATH, 0o777, exist_ok=True)
#
# try:
#   os.symlink(KAGGLE_INPUT_PATH, os.path.join("..", 'input'), target_is_directory=True)
# except FileExistsError:
#   pass
# try:
#   os.symlink(KAGGLE_WORKING_PATH, os.path.join("..", 'working'), target_is_directory=True)
# except FileExistsError:
#   pass
#
# for data_source_mapping in DATA_SOURCE_MAPPING.split(','):
#     directory, download_url_encoded = data_source_mapping.split(':')
#     download_url = unquote(download_url_encoded)
#     filename = urlparse(download_url).path
#     destination_path = os.path.join(KAGGLE_INPUT_PATH, directory)
#     try:
#         with urlopen(download_url) as fileres, NamedTemporaryFile() as tfile:
#             total_length = fileres.headers['content-length']
#             print(f'Downloading {directory}, {total_length} bytes compressed')
#             dl = 0
#             data = fileres.read(CHUNK_SIZE)
#             while len(data) > 0:
#                 dl += len(data)
#                 tfile.write(data)
#                 done = int(50 * dl / int(total_length))
#                 sys.stdout.write(f"\r[{'=' * done}{' ' * (50-done)}] {dl} bytes downloaded")
#                 sys.stdout.flush()
#                 data = fileres.read(CHUNK_SIZE)
#             if filename.endswith('.zip'):
#               with ZipFile(tfile) as zfile:
#                 zfile.extractall(destination_path)
#             else:
#               with tarfile.open(tfile.name) as tarfile:
#                 tarfile.extractall(destination_path)
#             print(f'\nDownloaded and uncompressed: {directory}')
#     except HTTPError as e:
#         print(f'Failed to load (likely expired) {download_url} to path {destination_path}')
#         continue
#     except OSError as e:
#         print(f'Failed to load {download_url} to path {destination_path}')
#         continue
#
# print('Data source import complete.')



import torch
from torch.utils.data import Dataset, DataLoader
import json
from PIL import Image
import os
import hashlib
import urllib
import warnings
from packaging import version
from typing import Union, List , Tuple
from torchvision.transforms import Compose, Resize, CenterCrop, ToTensor, Normalize
from tqdm import tqdm
from collections import OrderedDict
import numpy as np
import torch.nn.functional as F
from torch import nn
import gzip
import html
from functools import lru_cache
import ftfy
import regex as re
from IPython.display import display
from tqdm import tqdm
from torch.nn.utils.rnn import pad_sequence



try:
    from torchvision.transforms import InterpolationMode
    BICUBIC = InterpolationMode.BICUBIC
except ImportError:
    BICUBIC = Image.BICUBIC


if version.parse(torch.__version__) < version.parse("1.7.1"):
    warnings.warn("PyTorch version 1.7.1 or higher is recommended")

"""# **CLIP Model Methods:**

**MODEL CODE:**
"""

class Bottleneck(nn.Module):
    expansion = 4

    def __init__(self, inplanes, planes, stride=1):
        super().__init__()

        # all conv layers have stride 1. an avgpool is performed after the second convolution when stride > 1
        self.conv1 = nn.Conv2d(inplanes, planes, 1, bias=False)
        self.bn1 = nn.BatchNorm2d(planes)
        self.relu1 = nn.ReLU(inplace=True)

        self.conv2 = nn.Conv2d(planes, planes, 3, padding=1, bias=False)
        self.bn2 = nn.BatchNorm2d(planes)
        self.relu2 = nn.ReLU(inplace=True)

        self.avgpool = nn.AvgPool2d(stride) if stride > 1 else nn.Identity()

        self.conv3 = nn.Conv2d(planes, planes * self.expansion, 1, bias=False)
        self.bn3 = nn.BatchNorm2d(planes * self.expansion)
        self.relu3 = nn.ReLU(inplace=True)

        self.downsample = None
        self.stride = stride

        if stride > 1 or inplanes != planes * Bottleneck.expansion:
            # downsampling layer is prepended with an avgpool, and the subsequent convolution has stride 1
            self.downsample = nn.Sequential(OrderedDict([
                ("-1", nn.AvgPool2d(stride)),
                ("0", nn.Conv2d(inplanes, planes * self.expansion, 1, stride=1, bias=False)),
                ("1", nn.BatchNorm2d(planes * self.expansion))
            ]))

    def forward(self, x: torch.Tensor):
        identity = x

        out = self.relu1(self.bn1(self.conv1(x)))
        out = self.relu2(self.bn2(self.conv2(out)))
        out = self.avgpool(out)
        out = self.bn3(self.conv3(out))

        if self.downsample is not None:
            identity = self.downsample(x)

        out += identity
        out = self.relu3(out)
        return out


class AttentionPool2d(nn.Module):
    def __init__(self, spacial_dim: int, embed_dim: int, num_heads: int, output_dim: int = None):
        super().__init__()
        self.positional_embedding = nn.Parameter(torch.randn(spacial_dim ** 2 + 1, embed_dim) / embed_dim ** 0.5)
        self.k_proj = nn.Linear(embed_dim, embed_dim)
        self.q_proj = nn.Linear(embed_dim, embed_dim)
        self.v_proj = nn.Linear(embed_dim, embed_dim)
        self.c_proj = nn.Linear(embed_dim, output_dim or embed_dim)
        self.num_heads = num_heads

    def forward(self, x):
        x = x.flatten(start_dim=2).permute(2, 0, 1)  # NCHW -> (HW)NC
        x = torch.cat([x.mean(dim=0, keepdim=True), x], dim=0)  # (HW+1)NC
        x = x + self.positional_embedding[:, None, :].to(x.dtype)  # (HW+1)NC
        x, _ = F.multi_head_attention_forward(
            query=x[:1], key=x, value=x,
            embed_dim_to_check=x.shape[-1],
            num_heads=self.num_heads,
            q_proj_weight=self.q_proj.weight,
            k_proj_weight=self.k_proj.weight,
            v_proj_weight=self.v_proj.weight,
            in_proj_weight=None,
            in_proj_bias=torch.cat([self.q_proj.bias, self.k_proj.bias, self.v_proj.bias]),
            bias_k=None,
            bias_v=None,
            add_zero_attn=False,
            dropout_p=0,
            out_proj_weight=self.c_proj.weight,
            out_proj_bias=self.c_proj.bias,
            use_separate_proj_weight=True,
            training=self.training,
            need_weights=False
        )
        return x.squeeze(0)


class ModifiedResNet(nn.Module):
    """
    A ResNet class that is similar to torchvision's but contains the following changes:
    - There are now 3 "stem" convolutions as opposed to 1, with an average pool instead of a max pool.
    - Performs anti-aliasing strided convolutions, where an avgpool is prepended to convolutions with stride > 1
    - The final pooling layer is a QKV attention instead of an average pool
    """

    def __init__(self, layers, output_dim, heads, input_resolution=224, width=64):
        super().__init__()
        self.output_dim = output_dim
        self.input_resolution = input_resolution

        # the 3-layer stem
        self.conv1 = nn.Conv2d(3, width // 2, kernel_size=3, stride=2, padding=1, bias=False)
        self.bn1 = nn.BatchNorm2d(width // 2)
        self.relu1 = nn.ReLU(inplace=True)
        self.conv2 = nn.Conv2d(width // 2, width // 2, kernel_size=3, padding=1, bias=False)
        self.bn2 = nn.BatchNorm2d(width // 2)
        self.relu2 = nn.ReLU(inplace=True)
        self.conv3 = nn.Conv2d(width // 2, width, kernel_size=3, padding=1, bias=False)
        self.bn3 = nn.BatchNorm2d(width)
        self.relu3 = nn.ReLU(inplace=True)
        self.avgpool = nn.AvgPool2d(2)

        # residual layers
        self._inplanes = width  # this is a *mutable* variable used during construction
        self.layer1 = self._make_layer(width, layers[0])
        self.layer2 = self._make_layer(width * 2, layers[1], stride=2)
        self.layer3 = self._make_layer(width * 4, layers[2], stride=2)
        self.layer4 = self._make_layer(width * 8, layers[3], stride=2)

        embed_dim = width * 32  # the ResNet feature dimension
        self.attnpool = AttentionPool2d(input_resolution // 32, embed_dim, heads, output_dim)

    def _make_layer(self, planes, blocks, stride=1):
        layers = [Bottleneck(self._inplanes, planes, stride)]

        self._inplanes = planes * Bottleneck.expansion
        for _ in range(1, blocks):
            layers.append(Bottleneck(self._inplanes, planes))

        return nn.Sequential(*layers)

    def forward(self, x):
        def stem(x):
            x = self.relu1(self.bn1(self.conv1(x)))
            x = self.relu2(self.bn2(self.conv2(x)))
            x = self.relu3(self.bn3(self.conv3(x)))
            x = self.avgpool(x)
            return x

        x = x.type(self.conv1.weight.dtype)
        x = stem(x)
        x = self.layer1(x)
        x = self.layer2(x)
        x = self.layer3(x)
        x = self.layer4(x)
        x = self.attnpool(x)

        return x


class LayerNorm(nn.LayerNorm):
    """Subclass torch's LayerNorm to handle fp16."""

    def forward(self, x: torch.Tensor):
        orig_type = x.dtype
        ret = super().forward(x.type(torch.float32))
        return ret.type(orig_type)


class QuickGELU(nn.Module):
    def forward(self, x: torch.Tensor):
        return x * torch.sigmoid(1.702 * x)


class ResidualAttentionBlock(nn.Module):
    def __init__(self, d_model: int, n_head: int, attn_mask: torch.Tensor = None):
        super().__init__()

        self.attn = nn.MultiheadAttention(d_model, n_head)
        self.ln_1 = LayerNorm(d_model)
        self.mlp = nn.Sequential(OrderedDict([
            ("c_fc", nn.Linear(d_model, d_model * 4)),
            ("gelu", QuickGELU()),
            ("c_proj", nn.Linear(d_model * 4, d_model))
        ]))
        self.ln_2 = LayerNorm(d_model)
        self.attn_mask = attn_mask

    def attention(self, x: torch.Tensor):
        self.attn_mask = self.attn_mask.to(dtype=x.dtype, device=x.device) if self.attn_mask is not None else None
        return self.attn(x, x, x, need_weights=False, attn_mask=self.attn_mask)[0]

    def forward(self, x: torch.Tensor):
        x = x + self.attention(self.ln_1(x))
        x = x + self.mlp(self.ln_2(x))
        return x


class Transformer(nn.Module):
    def __init__(self, width: int, layers: int, heads: int, attn_mask: torch.Tensor = None):
        super().__init__()
        self.width = width
        self.layers = layers
        self.resblocks = nn.Sequential(*[ResidualAttentionBlock(width, heads, attn_mask) for _ in range(layers)])

    def forward(self, x: torch.Tensor):
        return self.resblocks(x)


class VisionTransformer(nn.Module):
    def __init__(self, input_resolution: int, patch_size: int, width: int, layers: int, heads: int, output_dim: int):
        super().__init__()
        self.input_resolution = input_resolution
        self.output_dim = output_dim
        self.conv1 = nn.Conv2d(in_channels=3, out_channels=width, kernel_size=patch_size, stride=patch_size, bias=False)

        scale = width ** -0.5
        self.class_embedding = nn.Parameter(scale * torch.randn(width))
        self.positional_embedding = nn.Parameter(scale * torch.randn((input_resolution // patch_size) ** 2 + 1, width))
        self.ln_pre = LayerNorm(width)

        self.transformer = Transformer(width, layers, heads)

        self.ln_post = LayerNorm(width)
        self.proj = nn.Parameter(scale * torch.randn(width, output_dim))

    def forward(self, x: torch.Tensor):
        x = self.conv1(x)  # shape = [*, width, grid, grid]
        x = x.reshape(x.shape[0], x.shape[1], -1)  # shape = [*, width, grid ** 2]
        x = x.permute(0, 2, 1)  # shape = [*, grid ** 2, width]
        x = torch.cat([self.class_embedding.to(x.dtype) + torch.zeros(x.shape[0], 1, x.shape[-1], dtype=x.dtype, device=x.device), x], dim=1)  # shape = [*, grid ** 2 + 1, width]
        x = x + self.positional_embedding.to(x.dtype)
        x = self.ln_pre(x)

        x = x.permute(1, 0, 2)  # NLD -> LND
        x = self.transformer(x)
        x = x.permute(1, 0, 2)  # LND -> NLD

        x = self.ln_post(x[:, 0, :])

        if self.proj is not None:
            x = x @ self.proj

        return x


class CLIP(nn.Module):
    def __init__(self,
                 embed_dim: int,
                 # vision
                 image_resolution: int,
                 vision_layers: Union[Tuple[int, int, int, int], int],
                 vision_width: int,
                 vision_patch_size: int,
                 # text
                 context_length: int,
                 vocab_size: int,
                 transformer_width: int,
                 transformer_heads: int,
                 transformer_layers: int
                 ):
        super().__init__()

        self.context_length = context_length

        if isinstance(vision_layers, (tuple, list)):
            vision_heads = vision_width * 32 // 64
            self.visual = ModifiedResNet(
                layers=vision_layers,
                output_dim=embed_dim,
                heads=vision_heads,
                input_resolution=image_resolution,
                width=vision_width
            )
        else:
            vision_heads = vision_width // 64
            self.visual = VisionTransformer(
                input_resolution=image_resolution,
                patch_size=vision_patch_size,
                width=vision_width,
                layers=vision_layers,
                heads=vision_heads,
                output_dim=embed_dim
            )

        self.transformer = Transformer(
            width=transformer_width,
            layers=transformer_layers,
            heads=transformer_heads,
            attn_mask=self.build_attention_mask()
        )

        self.vocab_size = vocab_size
        self.token_embedding = nn.Embedding(vocab_size, transformer_width)
        self.positional_embedding = nn.Parameter(torch.empty(self.context_length, transformer_width))
        self.ln_final = LayerNorm(transformer_width)

        self.text_projection = nn.Parameter(torch.empty(transformer_width, embed_dim))
        self.logit_scale = nn.Parameter(torch.ones([]) * np.log(1 / 0.07))

        self.initialize_parameters()

    def initialize_parameters(self):
        nn.init.normal_(self.token_embedding.weight, std=0.02)
        nn.init.normal_(self.positional_embedding, std=0.01)

        if isinstance(self.visual, ModifiedResNet):
            if self.visual.attnpool is not None:
                std = self.visual.attnpool.c_proj.in_features ** -0.5
                nn.init.normal_(self.visual.attnpool.q_proj.weight, std=std)
                nn.init.normal_(self.visual.attnpool.k_proj.weight, std=std)
                nn.init.normal_(self.visual.attnpool.v_proj.weight, std=std)
                nn.init.normal_(self.visual.attnpool.c_proj.weight, std=std)

            for resnet_block in [self.visual.layer1, self.visual.layer2, self.visual.layer3, self.visual.layer4]:
                for name, param in resnet_block.named_parameters():
                    if name.endswith("bn3.weight"):
                        nn.init.zeros_(param)

        proj_std = (self.transformer.width ** -0.5) * ((2 * self.transformer.layers) ** -0.5)
        attn_std = self.transformer.width ** -0.5
        fc_std = (2 * self.transformer.width) ** -0.5
        for block in self.transformer.resblocks:
            nn.init.normal_(block.attn.in_proj_weight, std=attn_std)
            nn.init.normal_(block.attn.out_proj.weight, std=proj_std)
            nn.init.normal_(block.mlp.c_fc.weight, std=fc_std)
            nn.init.normal_(block.mlp.c_proj.weight, std=proj_std)

        if self.text_projection is not None:
            nn.init.normal_(self.text_projection, std=self.transformer.width ** -0.5)

    def build_attention_mask(self):
        # lazily create causal attention mask, with full attention between the vision tokens
        # pytorch uses additive attention mask; fill with -inf
        mask = torch.empty(self.context_length, self.context_length)
        mask.fill_(float("-inf"))
        mask.triu_(1)  # zero out the lower diagonal
        return mask

    @property
    def dtype(self):
        return self.visual.conv1.weight.dtype

    def encode_image(self, image):
        return self.visual(image.type(self.dtype))

    def encode_text(self, text):
        x = self.token_embedding(text).type(self.dtype)  # [batch_size, n_ctx, d_model]

        x = x + self.positional_embedding.type(self.dtype)
        x = x.permute(1, 0, 2)  # NLD -> LND
        x = self.transformer(x)
        x = x.permute(1, 0, 2)  # LND -> NLD
        x = self.ln_final(x).type(self.dtype)

        # x.shape = [batch_size, n_ctx, transformer.width]
        # take features from the eot embedding (eot_token is the highest number in each sequence)
        x = x[torch.arange(x.shape[0]), text.argmax(dim=-1)] @ self.text_projection

        return x

    def forward(self, image, text):
        image_features = self.encode_image(image)
        text_features = self.encode_text(text)

        # normalized features
        image_features = image_features / image_features.norm(dim=1, keepdim=True)
        text_features = text_features / text_features.norm(dim=1, keepdim=True)

        # cosine similarity as logits
        logit_scale = self.logit_scale.exp()
        logits_per_image = logit_scale * image_features @ text_features.t()
        logits_per_text = logits_per_image.t()

        # shape = [global_batch_size, global_batch_size]
        return logits_per_image, logits_per_text


def convert_weights(model: nn.Module):
    """Convert applicable model parameters to fp16"""

    def _convert_weights_to_fp16(l):
        if isinstance(l, (nn.Conv1d, nn.Conv2d, nn.Linear)):
            l.weight.data = l.weight.data.half()
            if l.bias is not None:
                l.bias.data = l.bias.data.half()

        if isinstance(l, nn.MultiheadAttention):
            for attr in [*[f"{s}_proj_weight" for s in ["in", "q", "k", "v"]], "in_proj_bias", "bias_k", "bias_v"]:
                tensor = getattr(l, attr)
                if tensor is not None:
                    tensor.data = tensor.data.half()

        for name in ["text_projection", "proj"]:
            if hasattr(l, name):
                attr = getattr(l, name)
                if attr is not None:
                    attr.data = attr.data.half()

    model.apply(_convert_weights_to_fp16)


def build_model(state_dict: dict):
    vit = "visual.proj" in state_dict

    if vit:
        vision_width = state_dict["visual.conv1.weight"].shape[0]
        vision_layers = len([k for k in state_dict.keys() if k.startswith("visual.") and k.endswith(".attn.in_proj_weight")])
        vision_patch_size = state_dict["visual.conv1.weight"].shape[-1]
        grid_size = round((state_dict["visual.positional_embedding"].shape[0] - 1) ** 0.5)
        image_resolution = vision_patch_size * grid_size
    else:
        counts: list = [len(set(k.split(".")[2] for k in state_dict if k.startswith(f"visual.layer{b}"))) for b in [1, 2, 3, 4]]
        vision_layers = tuple(counts)
        vision_width = state_dict["visual.layer1.0.conv1.weight"].shape[0]
        output_width = round((state_dict["visual.attnpool.positional_embedding"].shape[0] - 1) ** 0.5)
        vision_patch_size = None
        assert output_width ** 2 + 1 == state_dict["visual.attnpool.positional_embedding"].shape[0]
        image_resolution = output_width * 32

    embed_dim = state_dict["text_projection"].shape[1]
    context_length = state_dict["positional_embedding"].shape[0]
    vocab_size = state_dict["token_embedding.weight"].shape[0]
    transformer_width = state_dict["ln_final.weight"].shape[0]
    transformer_heads = transformer_width // 64
    transformer_layers = len(set(k.split(".")[2] for k in state_dict if k.startswith("transformer.resblocks")))

    model = CLIP(
        embed_dim,
        image_resolution, vision_layers, vision_width, vision_patch_size,
        context_length, vocab_size, transformer_width, transformer_heads, transformer_layers
    )

    for key in ["input_resolution", "context_length", "vocab_size"]:
        if key in state_dict:
            del state_dict[key]

    convert_weights(model)
    model.load_state_dict(state_dict)
    return model.eval()

"""**TOKENIZER CODE:**"""

File_directory="D:/Downloads/bpe_simple_vocab_16e6.txt"
@lru_cache()
def default_bpe():
    return os.path.join(File_directory,"bpe_simple_vocab_16e6.txt")


@lru_cache()
def bytes_to_unicode():
    """
    Returns list of utf-8 byte and a corresponding list of unicode strings.
    The reversible bpe codes work on unicode strings.
    This means you need a large # of unicode characters in your vocab if you want to avoid UNKs.
    When you're at something like a 10B token dataset you end up needing around 5K for decent coverage.
    This is a signficant percentage of your normal, say, 32K bpe vocab.
    To avoid that, we want lookup tables between utf-8 bytes and unicode strings.
    And avoids mapping to whitespace/control characters the bpe code barfs on.
    """
    bs = list(range(ord("!"), ord("~")+1))+list(range(ord("¡"), ord("¬")+1))+list(range(ord("®"), ord("ÿ")+1))
    cs = bs[:]
    n = 0
    for b in range(2**8):
        if b not in bs:
            bs.append(b)
            cs.append(2**8+n)
            n += 1
    cs = [chr(n) for n in cs]
    return dict(zip(bs, cs))


def get_pairs(word):
    """Return set of symbol pairs in a word.
    Word is represented as tuple of symbols (symbols being variable-length strings).
    """
    pairs = set()
    prev_char = word[0]
    for char in word[1:]:
        pairs.add((prev_char, char))
        prev_char = char
    return pairs


def basic_clean(text):
    text = ftfy.fix_text(text)
    text = html.unescape(html.unescape(text))
    return text.strip()


def whitespace_clean(text):
    text = re.sub(r'\s+', ' ', text)
    text = text.strip()
    return text


class SimpleTokenizer(object):
    def __init__(self, bpe_path: str = default_bpe()):
        self.byte_encoder = bytes_to_unicode()
        self.byte_decoder = {v: k for k, v in self.byte_encoder.items()}
          # Open and read the BPE file correctly
        with open(bpe_path, "r", encoding="utf-8") as file:
            merges = file.read().split('\n')
#          merges = (bpe_path).read().decode("utf-8").split('\n')
        merges = merges[1:49152-256-2+1]
        merges = [tuple(merge.split()) for merge in merges]
        vocab = list(bytes_to_unicode().values())
        vocab = vocab + [v+'</w>' for v in vocab]
        for merge in merges:
            vocab.append(''.join(merge))
        vocab.extend(['<|startoftext|>', '<|endoftext|>'])
        self.encoder = dict(zip(vocab, range(len(vocab))))
        self.decoder = {v: k for k, v in self.encoder.items()}
        self.bpe_ranks = dict(zip(merges, range(len(merges))))
        self.cache = {'<|startoftext|>': '<|startoftext|>', '<|endoftext|>': '<|endoftext|>'}
        self.pat = re.compile(r"""<\|startoftext\|>|<\|endoftext\|>|'s|'t|'re|'ve|'m|'ll|'d|[\p{L}]+|[\p{N}]|[^\s\p{L}\p{N}]+""", re.IGNORECASE)

    def bpe(self, token):
        if token in self.cache:
            return self.cache[token]
        word = tuple(token[:-1]) + ( token[-1] + '</w>',)
        pairs = get_pairs(word)

        if not pairs:
            return token+'</w>'

        while True:
            bigram = min(pairs, key = lambda pair: self.bpe_ranks.get(pair, float('inf')))
            if bigram not in self.bpe_ranks:
                break
            first, second = bigram
            new_word = []
            i = 0
            while i < len(word):
                try:
                    j = word.index(first, i)
                    new_word.extend(word[i:j])
                    i = j
                except:
                    new_word.extend(word[i:])
                    break

                if word[i] == first and i < len(word)-1 and word[i+1] == second:
                    new_word.append(first+second)
                    i += 2
                else:
                    new_word.append(word[i])
                    i += 1
            new_word = tuple(new_word)
            word = new_word
            if len(word) == 1:
                break
            else:
                pairs = get_pairs(word)
        word = ' '.join(word)
        self.cache[token] = word
        return word

    def encode(self, text):
        bpe_tokens = []
        text = whitespace_clean(basic_clean(text)).lower()
        for token in re.findall(self.pat, text):
            token = ''.join(self.byte_encoder[b] for b in token.encode('utf-8'))
            bpe_tokens.extend(self.encoder[bpe_token] for bpe_token in self.bpe(token).split(' '))
        return bpe_tokens

    def decode(self, tokens):
        text = ''.join([self.decoder[token] for token in tokens])
        text = bytearray([self.byte_decoder[c] for c in text]).decode('utf-8', errors="replace").replace('</w>', ' ')
        return text

"""**CLIP MODEL METHODS:**"""

__all__ = ["available_models", "load", "tokenize"]
_tokenizer = SimpleTokenizer()

_MODELS = {
    "RN50": "https://openaipublic.azureedge.net/clip/models/afeb0e10f9e5a86da6080e35cf09123aca3b358a0c3e3b6c78a7b63bc04b6762/RN50.pt",
    "RN101": "https://openaipublic.azureedge.net/clip/models/8fa8567bab74a42d41c5915025a8e4538c3bdbe8804a470a72f30b0d94fab599/RN101.pt",
    "RN50x4": "https://openaipublic.azureedge.net/clip/models/7e526bd135e493cef0776de27d5f42653e6b4c8bf9e0f653bb11773263205fdd/RN50x4.pt",
    "RN50x16": "https://openaipublic.azureedge.net/clip/models/52378b407f34354e150460fe41077663dd5b39c54cd0bfd2b27167a4a06ec9aa/RN50x16.pt",
    "RN50x64": "https://openaipublic.azureedge.net/clip/models/be1cfb55d75a9666199fb2206c106743da0f6468c9d327f3e0d0a543a9919d9c/RN50x64.pt",
    "ViT-B/32": "https://openaipublic.azureedge.net/clip/models/40d365715913c9da98579312b702a82c18be219cc2a73407c4526f58eba950af/ViT-B-32.pt",
    "ViT-B/16": "https://openaipublic.azureedge.net/clip/models/5806e77cd80f8b59890b7e101eabd078d9fb84e6937f9e85e4ecb61988df416f/ViT-B-16.pt",
    "ViT-L/14": "https://openaipublic.azureedge.net/clip/models/b8cca3fd41ae0c99ba7e8951adf17d267cdb84cd88be6f7c2e0eca1737a03836/ViT-L-14.pt",
    "ViT-L/14@336px": "https://openaipublic.azureedge.net/clip/models/3035c92b350959924f9f00213499208652fc7ea050643e8b385c2dac08641f02/ViT-L-14-336px.pt",
}


def _download(url: str, root: str):
    os.makedirs(root, exist_ok=True)
    filename = os.path.basename(url)

    expected_sha256 = url.split("/")[-2]
    download_target = os.path.join(root, filename)

    if os.path.exists(download_target) and not os.path.isfile(download_target):
        raise RuntimeError(f"{download_target} exists and is not a regular file")

    if os.path.isfile(download_target):
        if hashlib.sha256(open(download_target, "rb").read()).hexdigest() == expected_sha256:
            return download_target
        else:
            warnings.warn(f"{download_target} exists, but the SHA256 checksum does not match; re-downloading the file")

    with urllib.request.urlopen(url) as source, open(download_target, "wb") as output:
        with tqdm(total=int(source.info().get("Content-Length")), ncols=80, unit='iB', unit_scale=True, unit_divisor=1024) as loop:
            while True:
                buffer = source.read(8192)
                if not buffer:
                    break

                output.write(buffer)
                loop.update(len(buffer))

    if hashlib.sha256(open(download_target, "rb").read()).hexdigest() != expected_sha256:
        raise RuntimeError("Model has been downloaded but the SHA256 checksum does not not match")

    return download_target


def _convert_image_to_rgb(image):
    return image.convert("RGB")


def _transform(n_px):
    return Compose([
        Resize(n_px, interpolation=BICUBIC),
        CenterCrop(n_px),
        _convert_image_to_rgb,
        ToTensor(),
        Normalize((0.48145466, 0.4578275, 0.40821073), (0.26862954, 0.26130258, 0.27577711)),
    ])


def available_models() -> List[str]:
    """Returns the names of available CLIP models"""
    return list(_MODELS.keys())


def load(name: str, device: Union[str, torch.device] = "cuda" if torch.cuda.is_available() else "cpu", jit: bool = False, download_root: str = None):
    """Load a CLIP model

    Parameters
    ----------
    name : str
        A model name listed by `clip.available_models()`, or the path to a model checkpoint containing the state_dict

    device : Union[str, torch.device]
        The device to put the loaded model

    jit : bool
        Whether to load the optimized JIT model or more hackable non-JIT model (default).

    download_root: str
        path to download the model files; by default, it uses "~/.cache/clip"

    Returns
    -------
    model : torch.nn.Module
        The CLIP model

    preprocess : Callable[[PIL.Image], torch.Tensor]
        A torchvision transform that converts a PIL image into a tensor that the returned model can take as its input
    """
    if name in _MODELS:
        model_path = _download(_MODELS[name], download_root or os.path.expanduser("~/.cache/clip"))
    elif os.path.isfile(name):
        model_path = name
    else:
        raise RuntimeError(f"Model {name} not found; available models = {available_models()}")

    with open(model_path, 'rb') as opened_file:
        try:
            # loading JIT archive
            model = torch.jit.load(opened_file, map_location=device if jit else "cpu").eval()
            state_dict = None
        except RuntimeError:
            # loading saved state dict
            if jit:
                warnings.warn(f"File {model_path} is not a JIT archive. Loading as a state dict instead")
                jit = False
            state_dict = torch.load(opened_file, map_location="cpu")

    if not jit:
        model = build_model(state_dict or model.state_dict()).to(device)
        if str(device) == "cpu":
            model.float()
        return model, _transform(model.visual.input_resolution)

    # patch the device names
    device_holder = torch.jit.trace(lambda: torch.ones([]).to(torch.device(device)), example_inputs=[])
    device_node = [n for n in device_holder.graph.findAllNodes("prim::Constant") if "Device" in repr(n)][-1]

    def _node_get(node: torch._C.Node, key: str):
        """Gets attributes of a node which is polymorphic over return type.

        From https://github.com/pytorch/pytorch/pull/82628
        """
        sel = node.kindOf(key)
        return getattr(node, sel)(key)

    def patch_device(module):
        try:
            graphs = [module.graph] if hasattr(module, "graph") else []
        except RuntimeError:
            graphs = []

        if hasattr(module, "forward1"):
            graphs.append(module.forward1.graph)

        for graph in graphs:
            for node in graph.findAllNodes("prim::Constant"):
                if "value" in node.attributeNames() and str(_node_get(node, "value")).startswith("cuda"):
                    node.copyAttributes(device_node)

    model.apply(patch_device)
    patch_device(model.encode_image)
    patch_device(model.encode_text)

    # patch dtype to float32 on CPU
    if str(device) == "cpu":
        float_holder = torch.jit.trace(lambda: torch.ones([]).float(), example_inputs=[])
        float_input = list(float_holder.graph.findNode("aten::to").inputs())[1]
        float_node = float_input.node()

        def patch_float(module):
            try:
                graphs = [module.graph] if hasattr(module, "graph") else []
            except RuntimeError:
                graphs = []

            if hasattr(module, "forward1"):
                graphs.append(module.forward1.graph)

            for graph in graphs:
                for node in graph.findAllNodes("aten::to"):
                    inputs = list(node.inputs())
                    for i in [1, 2]:  # dtype can be the second or third argument to aten::to()
                        if _node_get(inputs[i].node(), "value") == 5:
                            inputs[i].node().copyAttributes(float_node)

        model.apply(patch_float)
        patch_float(model.encode_image)
        patch_float(model.encode_text)

        model.float()

    return model, _transform(model.input_resolution.item())


def tokenize(texts: Union[str, List[str]], context_length: int = 77, truncate: bool = False) -> Union[torch.IntTensor, torch.LongTensor]:
    """
    Returns the tokenized representation of given input string(s)

    Parameters
    ----------
    texts : Union[str, List[str]]
        An input string or a list of input strings to tokenize

    context_length : int
        The context length to use; all CLIP models use 77 as the context length

    truncate: bool
        Whether to truncate the text in case its encoding is longer than the context length

    Returns
    -------
    A two-dimensional tensor containing the resulting tokens, shape = [number of input strings, context_length].
    We return LongTensor when torch version is <1.8.0, since older index_select requires indices to be long.
    """
    if isinstance(texts, str):
        texts = [texts]

    sot_token = _tokenizer.encoder["<|startoftext|>"]
    eot_token = _tokenizer.encoder["<|endoftext|>"]
    all_tokens = [[sot_token] + _tokenizer.encode(text) + [eot_token] for text in texts]
    if version.parse(torch.__version__) < version.parse("1.8.0"):
        result = torch.zeros(len(all_tokens), context_length, dtype=torch.long)
    else:
        result = torch.zeros(len(all_tokens), context_length, dtype=torch.int)

    for i, tokens in enumerate(all_tokens):
        if len(tokens) > context_length:
            if truncate:
                tokens = tokens[:context_length]
                tokens[-1] = eot_token
            else:
                raise RuntimeError(f"Input {texts[i]} is too long for context length {context_length}")
        result[i, :len(tokens)] = torch.tensor(tokens)

    return result

"""# **CLIP EMBEDDINGS:**"""

# device = "cuda" if torch.cuda.is_available() else "cpu"
# model, preprocess = load("ViT-B/32", device=device)


# # Load COCO dataset
# data_dir_2='/kaggle/input/coco-image-caption/annotations_trainval2014/annotations'
# annotation_file = os.path.join(data_dir_2, 'captions_train2014.json')

# with open(annotation_file, 'r') as f:
#     coco_annotations = json.load(f)

# # Directory containing images
# image_dir = '/kaggle/input/coco-image-caption/train2014/train2014'  # Update this to your images directory

# text_embeddings=[]
# image_embeddings=[]

# # # Convert Tensor to PIL Image
# # def tensor_to_pil(tensor):
# #     tensor = tensor.squeeze().cpu().detach().clamp(0, 1)
# #     array = tensor.permute(1, 2, 0).numpy()
# #     return Image.fromarray((array * 255).astype(np.uint8))


# # Process each image and its captions
# for image_info in tqdm(coco_annotations['images'], desc="Processing Images"):
# # for i, image_info in enumerate(tqdm(coco_annotations['images'], desc="Processing Images")):
# #     if i >= 10:
# #         break  # Process only the first 10 images

#     image_id = image_info['id']
#     image_file = image_info['file_name']
#     image_path = os.path.join(image_dir, image_file)

#     # Preprocess the image
#     image = preprocess(Image.open(image_path)).unsqueeze(0).to(device)

#     # Get captions for this image
#     captions = [ann['caption'] for ann in coco_annotations['annotations'] if ann['image_id'] == image_id]

#     # Tokenize captions
#     text = tokenize(captions).to(device)

#     # Compute image and text features
#     with torch.no_grad():
#         image_features = model.encode_image(image)
#         text_features = model.encode_text(text)

#     image_embeddings.append(image_features)
#     text_embeddings.append(text_features)

# Paths to the directories containing text and image embeddings
text_embeddings_folder = 'D:/Downloads/text_embeddings_stored_2'
image_embeddings_folder = 'D:/Downloads/image_embeddings_stored_2'

# Lists to store all the loaded embeddings
text_embeddings = []
image_embeddings = []

# # Function to load embeddings from a folder
# def load_embeddings(folder_path, embeddings_list):
#     for file_name in os.listdir(folder_path):
#         if file_name.endswith('.pt'):
#             file_path = os.path.join(folder_path, file_name)

#             # Load each .pt file and append the embedding to the respective list
#             embedding = torch.load(file_path)
#             embeddings_list.append(embedding)
# Function to load embeddings from a folder with progress bar
def load_embeddings(folder_path, embeddings_list):
    files = [f for f in os.listdir(folder_path) if f.endswith('.pt')]

    for file_name in tqdm(files, desc=f"Loading from {folder_path}"):
        file_path = os.path.join(folder_path, file_name)

        # Load each .pt file and append the embedding to the respective list
        embedding = torch.load(file_path)
        embeddings_list.append(embedding)


# Load text embeddings
load_embeddings(text_embeddings_folder, text_embeddings)
print(f"Loaded {len(text_embeddings)} text embeddings.")

# Load image embeddings
load_embeddings(image_embeddings_folder, image_embeddings)
print(f"Loaded {len(image_embeddings)} image embeddings.")

class CLIPDataset(Dataset):
    def __init__(self, image_embeddings, text_embeddings):
        self.image_embeddings = image_embeddings
        self.text_embeddings = text_embeddings

    def __len__(self):
        return len(self.image_embeddings)

    def __getitem__(self, idx):
        image_embedding = self.image_embeddings[idx]
        text_embedding = self.text_embeddings[idx]

        # Ensure they are tensors
        if not isinstance(image_embedding, torch.Tensor):
            image_embedding = torch.tensor(image_embedding, dtype=torch.float32)

        if not isinstance(text_embedding, torch.Tensor):
            text_embedding = torch.tensor(text_embedding, dtype=torch.float32)

        return text_embedding, image_embedding


def custom_collate(batch):
    text_embeddings, image_embeddings = zip(*batch)

    # Pad text embeddings to have the same length
    text_embeddings_padded = pad_sequence(text_embeddings, batch_first=True)

    # Stack image embeddings (assuming they are all the same size)
    image_embeddings = torch.stack(image_embeddings, dim=0)

    return text_embeddings_padded, image_embeddings



# Create dataset
dataset = CLIPDataset(image_embeddings, text_embeddings)

# Use the custom collate function in your DataLoader
dataloader = torch.utils.data.DataLoader(
    dataset,
    batch_size=1024,
    collate_fn=custom_collate,
    shuffle=True
)

# Create DataLoader
#dataloader = DataLoader(dataset, batch_size=1024, shuffle=True)

image_embeddings[0].shape

import torch
import torch.nn as nn
import torch.nn.functional as F

# Positional Encoding for timesteps (used in diffusion models)
class PositionalEncoding(nn.Module):
    def __init__(self, d_model, max_len=1000):
        super(PositionalEncoding, self).__init__()
        pe = torch.zeros(max_len, d_model)
        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)
        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-torch.log(torch.tensor(10000.0)) / d_model))
        pe[:, 0::2] = torch.sin(position * div_term)
        pe[:, 1::2] = torch.cos(position * div_term)
        pe = pe.unsqueeze(0).transpose(0, 1)
        self.register_buffer('pe', pe)

    def forward(self, x, timestep):
        """
        :param x: Input tensor (batch_size, embedding_dim)
        :param timestep: Tensor representing timesteps (batch_size,)
        :return: Tensor with positional encoding added to input (batch_size, embedding_dim)
        """
        return x + self.pe[timestep].squeeze(1)

# Transformer Encoder Block
class TransformerBlock(nn.Module):
    def __init__(self, embedding_dim, num_heads, hidden_dim):
        super(TransformerBlock, self).__init__()
        self.attn = nn.MultiheadAttention(embedding_dim, num_heads)
        self.ffn = nn.Sequential(
            nn.Linear(embedding_dim, hidden_dim),
            nn.ReLU(),
            nn.Linear(hidden_dim, embedding_dim)
        )
        self.ln1 = nn.LayerNorm(embedding_dim)
        self.ln2 = nn.LayerNorm(embedding_dim)

    def forward(self, x):
        # Self-attention layer
        attn_output, _ = self.attn(x, x, x)
        x = self.ln1(x + attn_output)

        # Feedforward network
        ffn_output = self.ffn(x)
        x = self.ln2(x + ffn_output)

        return x

# # Diffusion Prior Model
# class DiffusionPrior(nn.Module):
#     def __init__(self, text_embedding_dim, image_embedding_dim, hidden_dim, num_layers, num_heads):
#         super(DiffusionPrior, self).__init__()

#         # Embedding layer to match text and image embeddings with model's hidden dim
#         self.text_embedding_proj = nn.Linear(text_embedding_dim, hidden_dim)
#         self.image_embedding_proj = nn.Linear(image_embedding_dim, hidden_dim)

#         # Positional encoding for timesteps
#         self.positional_encoding = PositionalEncoding(hidden_dim)

#         # Transformer layers for processing the embeddings
#         self.transformer_blocks = nn.ModuleList([
#             TransformerBlock(hidden_dim, num_heads, hidden_dim) for _ in range(num_layers)
#         ])

#         # Final layer to project the hidden state back to image embedding size
#         self.output_proj = nn.Linear(hidden_dim, image_embedding_dim)

#     def forward(self, text_embeddings, noised_image_embeddings, timesteps):
#         """
#         :param text_embeddings: Tensor of shape (batch_size, text_embedding_dim)
#         :param noised_image_embeddings: Tensor of shape (batch_size, image_embedding_dim)
#         :param timesteps: Tensor of shape (batch_size,)
#         :return: Predicted image embeddings (batch_size, image_embedding_dim)
#         """
#         # Project text and noised image embeddings into model's hidden dimension
#         text_embeddings = self.text_embedding_proj(text_embeddings)
#         noised_image_embeddings = self.image_embedding_proj(noised_image_embeddings)

#         # Combine text and noised image embeddings
#         combined_embeddings = text_embeddings + noised_image_embeddings

#         # Add positional encoding for timesteps
#         combined_embeddings = self.positional_encoding(combined_embeddings, timesteps)

#         # Prepare for transformer (batch_size, seq_len=1, hidden_dim)
#         combined_embeddings = combined_embeddings.unsqueeze(1)

#         # Pass through transformer layers
#         for transformer_block in self.transformer_blocks:
#             combined_embeddings = transformer_block(combined_embeddings)

#         # Remove the seq_len dimension and project to output size (image embedding dim)
#         combined_embeddings = combined_embeddings.squeeze(1)
#         predicted_image_embedding = self.output_proj(combined_embeddings)

#         return predicted_image_embedding



class DiffusionPrior(nn.Module):
    def __init__(self, text_embedding_dim, image_embedding_dim, hidden_dim, num_layers, num_heads):
        super(DiffusionPrior, self).__init__()
        self.text_embedding_proj = nn.Linear(text_embedding_dim, hidden_dim)
        self.image_embedding_proj = nn.Linear(image_embedding_dim, hidden_dim)
        # Define a linear layer to project combined embeddings to the transformer's d_model
        self.combined_projection = nn.Linear(2 * hidden_dim, hidden_dim)

        self.transformer = nn.Transformer(
            d_model=hidden_dim,
            nhead=num_heads,
            num_encoder_layers=num_layers,
            num_decoder_layers=num_layers
        )
        self.output_proj = nn.Linear(hidden_dim, image_embedding_dim)

    def forward(self, text_embeddings, noised_image_embeddings, timesteps):
        # Project text and noised image embeddings into model's hidden dimension
        text_embeddings = self.text_embedding_proj(text_embeddings)
        noised_image_embeddings = self.image_embedding_proj(noised_image_embeddings)

        # Assume noised_image_embeddings needs to be adjusted to match text_embeddings sequence length
        if noised_image_embeddings.size(1) != text_embeddings.size(1):
            # Here you might need to align dimensions based on your model's requirement
            noised_image_embeddings = noised_image_embeddings.repeat(1, text_embeddings.size(1), 1)

        # Combine text and noised image embeddings
        combined_embeddings = torch.cat((text_embeddings, noised_image_embeddings), dim=-1)

        # Project combined embeddings to match transformer's d_model
        combined_embeddings = self.combined_projection(combined_embeddings)

        # Check combined embeddings' feature dimension
        assert combined_embeddings.size(-1) == self.transformer.d_model, \
            f"Feature dimension mismatch: Expected {self.transformer.d_model}, got {combined_embeddings.size(-1)}"

        # Pass through transformer
        transformer_output = self.transformer(combined_embeddings, combined_embeddings)

        # The CLIP-style embeddings will be the transformer output
        clip_embeddings = transformer_output

        # Project back to image embedding dimension
        predicted_image_embedding = self.output_proj(clip_embeddings)

        return clip_embeddings, predicted_image_embedding


def add_noise_to_image(image_embedding, device):
    image_embedding = image_embedding.to(torch.float32)
    # Check the number of dimensions
    if image_embedding.dim() == 3 and image_embedding.shape[1] == 1:
        batch_size, _, embedding_dim = image_embedding.shape
        noise = torch.randn_like(image_embedding) * 0.1  # Adjust noise level as needed
        noised_image_embedding = image_embedding + noise
    else:
        raise ValueError("Unsupported tensor shape for image_embedding")

    # Simulate timestep embeddings (replace with actual timesteps in your model)
    timesteps = torch.randint(0, 1000, (batch_size,), device=device)  # Example timesteps

    return noised_image_embedding, timesteps


# Example loss function for diffusion (replace with your actual loss)
def diffusion_loss(predicted_embedding, true_embedding):
    return torch.nn.functional.mse_loss(predicted_embedding, true_embedding)

clip_embeddings_list = []  # List to store CLIP embeddings


# Define the training loop and store the CLIP embeddings
def train_diffusion_prior(model, data_loader, optimizer, epochs=10, device="cuda"):
    model.train()



    for epoch in range(epochs):
        total_loss = 0

        for text_embeddings, true_image_embeddings in tqdm(data_loader, desc=f"Epoch {epoch+1}/{epochs}"):
            optimizer.zero_grad()

            # Move data to the correct device (GPU/CPU)
            text_embeddings = text_embeddings.to(device).to(torch.float32)
            true_image_embeddings = true_image_embeddings.to(device).to(torch.float32)

            # Generate noised image embeddings (for diffusion model training)
            noised_image_embeddings, timesteps = add_noise_to_image(true_image_embeddings, device)

            # Forward pass through the model
            clip_embeddings, predicted_image_embedding = model(text_embeddings, noised_image_embeddings, timesteps)

            # Store CLIP embeddings for later use
            clip_embeddings_list.append(clip_embeddings.detach().cpu())

            # Compute the loss between predicted and true image embeddings
            loss = diffusion_loss(predicted_image_embedding, true_image_embeddings)

            # Backward pass and optimization
            loss.backward()
            optimizer.step()

            total_loss += loss.item()

        print(f"Epoch {epoch + 1}/{epochs}, Loss: {total_loss / len(data_loader)}")





# Assume your `DiffusionPrior` model is defined
# Initialize model, optimizer, and train

text_embedding_dim = 512  # Example dimension, adjust as per your CLIP model
image_embedding_dim = 512  # Example dimension
hidden_dim = 768
num_layers = 6
num_heads = 8

# Define device (GPU if available, otherwise CPU)
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

# Initialize your model (replace `DiffusionPrior` with your actual model class)
model = DiffusionPrior(text_embedding_dim, image_embedding_dim, hidden_dim, num_layers, num_heads).to(device)

# Initialize optimizer
optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)

# Train the model
train_diffusion_prior(model, dataloader, optimizer, epochs=10, device=device)

print(len(clip_embeddings_list))
# import numpy as np
# import torch

# # Load the NumPy file
# clip_embeddings_np_list = np.load('/kaggle/working/clip_embeddings.npy', allow_pickle=True)

# # Convert each NumPy array back to a PyTorch tensor
# clip_embeddings_list = [torch.from_numpy(array) for array in clip_embeddings_np_list]

import torch
import torch.nn as nn
from transformers import CLIPTokenizer, CLIPTextModel
from PIL import Image
import numpy as np
from tqdm import tqdm


device = "cuda" if torch.cuda.is_available() else "cpu"
model, preprocess = load("ViT-B/32", device=device)

# Define a simple U-Net-like model for diffusion (an alternative to GLIDE)
class UNet(nn.Module):
    def __init__(self, in_channels, out_channels):
        super(UNet, self).__init__()
        # Define a basic architecture
        self.conv1 = nn.Conv2d(in_channels, 64, kernel_size=3, padding=1)
        self.conv2 = nn.Conv2d(64, out_channels, kernel_size=3, padding=1)

    def forward(self, x):
        x = torch.relu(self.conv1(x))
        x = torch.sigmoid(self.conv2(x))
        return x


# Function to tokenize and encode text
def get_text_embeddings(prompt):
    tokens = tokenize(prompt)
    with torch.no_grad():
        text_embeddings = model.encode_text(tokens)
    print(f"Memory after get_text_embeddings: {torch.cuda.memory_allocated() / (1024 ** 3)} GB")
    return text_embeddings

# Define a simple forward diffusion model (alternative to the diffusion GLIDE uses)
class DiffusionModel(nn.Module):
    def __init__(self):
        super(DiffusionModel, self).__init__()
        self.unet = UNet(in_channels=3, out_channels=3)  # Simplified

    def forward(self, noise, combined_embeddings):
        # You can incorporate the embeddings into the forward pass as conditioning information
        return self.unet(noise)





# Function to combine multiple CLIP embeddings (list of tensors) and text embeddings
def combine_embeddings(clip_embeddings_list, text_embeddings):
    # Combine the CLIP embeddings (assuming they're a list of tensors)
    combined_clip_embeddings = torch.cat(clip_embeddings_list, dim=1)  # Concatenate along the feature dimension

    print(f"Memory after combine_embeddings 1: {torch.cuda.memory_allocated() / (1024 ** 3)} GB")

    # Reduce the text embeddings to match the CLIP embedding dimensions
    text_embeddings = text_embeddings.mean(dim=1)  # Reduce over sequence length to shape (batch_size, feature_dim)

    # Ensure combined_clip_embeddings and text_embeddings have matching dimensions
    clip_dim = combined_clip_embeddings.size(1)
    text_dim = text_embeddings.size(-1)

    # If dimensions don't match, adjust by padding or truncating the embeddings
    if clip_dim != text_dim:
        if clip_dim > text_dim:
            # Pad text_embeddings to match the CLIP dimension
            padding = torch.zeros(text_embeddings.size(0), clip_dim - text_dim).cuda()  # Create padding tensor
            text_embeddings = torch.cat([text_embeddings, padding], dim=-1)  # Concatenate along the last dimension
        else:
            # Truncate CLIP embeddings to match the text dimension
            combined_clip_embeddings = combined_clip_embeddings[:, :text_dim]  # Truncate CLIP embeddings

    # Combine the text and CLIP embeddings
    combined_embeddings = torch.cat([combined_clip_embeddings, text_embeddings], dim=-1)
    print(f"Memory after combined_embeddings 2: {torch.cuda.memory_allocated() / (1024 ** 3)} GB")
    return combined_embeddings



# Function to generate an image from noise using a diffusion process
def generate_image_from_embeddings(model, tokenizer, prompt, clip_embeddings_list):
    # Tokenize the text prompt
    text_embeddings = get_text_embeddings(prompt).cuda()

    # Combine the text and clip embeddings (from the list)
    combined_embeddings = combine_embeddings(clip_embeddings_list, text_embeddings)

    # Generate noise as a starting point for diffusion
    noise = torch.randn((1, 3, 64, 64)).cuda()  # 64x64 image, 3 channels (RGB)

    print(f"Memory after noise: {torch.cuda.memory_allocated() / (1024 ** 3)} GB")

    # Generate image using the diffusion model
    with torch.no_grad():
        generated_images = model(noise, combined_embeddings)
    print(f"Memory after generating image: {torch.cuda.memory_allocated() / (1024 ** 3)} GB")


    # Convert tensor to PIL image
    def tensor_to_pil(tensor):
        tensor = tensor.clamp(0, 1)  # Ensure values are within [0, 1]
        tensor = tensor.cpu().numpy().transpose(1, 2, 0)  # Convert to HWC
        return Image.fromarray((tensor * 255).astype(np.uint8))

    return tensor_to_pil(generated_images[0])  # Return the first image

def pad_or_truncate_tensor(tensor, target_size):
    # Ensure the tensor is on the same device
    device = tensor.device

    # Get the size of the last dimension
    current_size = tensor.size(-1)

    # If tensor is smaller, pad it
    if current_size < target_size:
        padding_size = target_size - current_size
        padding_shape = list(tensor.size())  # Get the shape of the tensor
        padding_shape[-1] = padding_size  # Set the padding size for the last dimension
        padding = torch.zeros(padding_shape).to(device)  # Create padding tensor
        tensor = torch.cat([tensor, padding], dim=-1)  # Concatenate along the last dimension

    # If tensor is larger, truncate it
    elif current_size > target_size:
        tensor = tensor[..., :target_size]  # Truncate along the last dimension

    return tensor



# Apply padding or truncation to all tensors in the clip_embeddings_list
target_size = 1024  # Ensure all tensors have the same size
clip_embeddings_list = [pad_or_truncate_tensor(tensor, target_size) for tensor in clip_embeddings_list]

# Initialize the diffusion model
diffusion_model = DiffusionModel().cuda()

# Example usage
prompt = "A boy sits on a white couch with a big smile"

# Generate image from prompt and CLIP embeddings (list)
generated_image = generate_image_from_embeddings(diffusion_model, tokenizer, prompt, clip_embeddings_list)

# Save or display the image
# generated_image.save("generated_image.png")
# generated_image.show()

import matplotlib.pyplot as plt
from PIL import Image

# Assuming `generated_image` is your image
def display_image(image):
    plt.imshow(image)
    plt.axis('off')  # Turn off axis labels
    plt.show()

# Display the generated image
display_image(generated_image)
